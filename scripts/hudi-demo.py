from pyspark import SparkConf
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

jars = "org.apache.hudi:hudi-spark3-bundle_2.12:1.0.2"

# Setup config
conf = (
    SparkConf()
    .setAppName("hudiDemo")
    .set(
        "spark.sql.extensions", "org.apache.spark.sql.hudi.HoodieSparkSessionExtension"
    )
    .set(
        "spark.sql.catalog.spark_catalog",
        "org.apache.spark.sql.hudi.catalog.HoodieCatalog",
    )
    .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    .set("spark.kryo.registrator", "org.apache.spark.HoodieSparkKryoRegistrar")
    .set("spark.jars.packages", jars)
)

# Create spark session
spark = SparkSession.builder.config(conf=conf).getOrCreate()
spark.sparkContext.setLogLevel("ERROR")

# Path
HOME = "/opt/spark/"

# Create table
tableName = "trips_table"
basePath = HOME + "warehouse/hudi/trips_table"
print(f"Creating table {tableName} at {basePath}")

# Insert data
print("Inserting data")
columns = ["ts", "uuid", "rider", "driver", "fare", "city"]
data = [
    (
        1695159649087,
        "334e26e9-8355-45cc-97c6-c31daf0df330",
        "rider-A",
        "driver-K",
        19.10,
        "san_francisco",
    ),
    (
        1695091554788,
        "e96c4396-3fad-413a-a942-4cb36106d721",
        "rider-C",
        "driver-M",
        27.70,
        "san_francisco",
    ),
    (
        1695046462179,
        "9909a8b1-2d15-4d3d-8ec9-efc48c536a00",
        "rider-D",
        "driver-L",
        33.90,
        "san_francisco",
    ),
    (
        1695516137016,
        "e3cf430c-889d-4015-bc98-59bdce1e530c",
        "rider-F",
        "driver-P",
        34.15,
        "sao_paulo",
    ),
    (
        1695115999911,
        "c8abbe79-8d89-47ea-b4ce-4d224bae5bfa",
        "rider-J",
        "driver-T",
        17.85,
        "chennai",
    ),
]
inserts = spark.createDataFrame(data).toDF(*columns)

hudi_options = {
    "hoodie.table.name": tableName,
    "hoodie.datasource.write.partitionpath.field": "city",
    "hoodie.datasource.write.recordkey.field": "uuid",
}

inserts.write.format("hudi").options(**hudi_options).mode("overwrite").save(basePath)

# Query data
print("Querying data")
tripsDF = spark.read.format("hudi").load(basePath)
tripsDF.createOrReplaceTempView("trips_table")

spark.sql(
    "SELECT uuid, fare, ts, rider, driver, city FROM  trips_table WHERE fare > 20.0"
).show()
spark.sql(
    "SELECT _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare FROM trips_table"
).show()

# Update data
print("Updating data")
# Lets read data from target Hudi table, modify fare column for rider-D and update it.
updatesDf = (
    spark.read.format("hudi")
    .load(basePath)
    .filter("rider == 'rider-D'")
    .withColumn("fare", col("fare") * 10)
)

updatesDf.write.format("hudi").options(**hudi_options).mode("append").save(basePath)

# Delete data
# Lets  delete rider: rider-F
print("Deleting data")
deletesDF = spark.read.format("hudi").load(basePath).filter("rider == 'rider-F'")

# issue deletes
hudi_hard_delete_options = {
    "hoodie.table.name": tableName,
    "hoodie.datasource.write.partitionpath.field": "city",
    "hoodie.datasource.write.operation": "delete",
}

deletesDF.write.format("hudi").options(**hudi_hard_delete_options).mode("append").save(
    basePath
)

# Incremental query
# reload data
spark.read.format("hudi").load(basePath).createOrReplaceTempView("trips_table")

commits = list(
    map(
        lambda row: row[0],
        spark.sql(
            "SELECT DISTINCT(_hoodie_commit_time) AS commitTime FROM  trips_table ORDER BY commitTime"
        )
        .limit(50)
        .collect(),
    )
)
beginTime = commits[len(commits) - 2]  # commit time we are interested in

print(f"Querying data incrementally from commit time: {beginTime}")
# incrementally query data
incremental_read_options = {
    "hoodie.datasource.query.type": "incremental",
    "hoodie.datasource.read.begin.instanttime": beginTime,
}

tripsIncrementalDF = (
    spark.read.format("hudi").options(**incremental_read_options).load(basePath)
)
tripsIncrementalDF.createOrReplaceTempView("trips_incremental")

spark.sql(
    "SELECT `_hoodie_commit_time`, fare, rider, driver, uuid, ts FROM trips_incremental WHERE fare > 20.0"
).show()

# Change Data Capture Query
print("Change Data Capture Query")
# Lets first insert data to a new table with cdc enabled.
columns = ["ts", "uuid", "rider", "driver", "fare", "city"]
data = [
    (
        1695159649087,
        "334e26e9-8355-45cc-97c6-c31daf0df330",
        "rider-A",
        "driver-K",
        19.10,
        "san_francisco",
    ),
    (
        1695091554788,
        "e96c4396-3fad-413a-a942-4cb36106d721",
        "rider-B",
        "driver-L",
        27.70,
        "san_francisco",
    ),
    (
        1695046462179,
        "9909a8b1-2d15-4d3d-8ec9-efc48c536a00",
        "rider-C",
        "driver-M",
        33.90,
        "san_francisco",
    ),
    (
        1695516137016,
        "e3cf430c-889d-4015-bc98-59bdce1e530c",
        "rider-C",
        "driver-N",
        34.15,
        "sao_paulo",
    ),
]

inserts = spark.createDataFrame(data).toDF(*columns)

hudi_options = {
    "hoodie.table.name": tableName,
    "hoodie.datasource.write.partitionpath.field": "city",
    "hoodie.table.cdc.enabled": "true",
    "hoodie.datasource.write.precombine.field": "ts",
    "hoodie.datasource.write.recordkey.field": "uuid",
    "hoodie.datasource.write.table.type": "MERGE_ON_READ",
}
# Insert data
inserts.write.format("hudi").options(**hudi_options).mode("overwrite").save(basePath)


#  Update fare for riders: rider-A and rider-B
updatesDf = (
    spark.read.format("hudi")
    .load(basePath)
    .filter("rider == 'rider-A' or rider == 'rider-B'")
    .withColumn("fare", col("fare") * 10)
)

updatesDf.write.format("hudi").mode("append").save(basePath)

# Query CDC data
cdc_read_options = {
    "hoodie.datasource.query.incremental.format": "cdc",
    "hoodie.datasource.query.type": "incremental",
    "hoodie.datasource.read.begin.instanttime": 0,
}
spark.read.format("hudi").options(**cdc_read_options).load(basePath).show(10, False)
